{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Python/')\n",
    "from Preprocessing import standardize\n",
    "from Evaluation import split_train_test_ma, train_model, test_model\n",
    "from Databases import get_databases_path, get_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases_path = '../../Databases/Sinteticas'\n",
    "paths = get_databases_path(databases_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the attributes and labels\n",
    "attr_df = get_database(paths[1], 'database_attr.csv')\n",
    "labels_df = get_database(paths[1], 'database_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the attributes\n",
    "standardized_attr_df = standardize(attr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = split_train_test_ma(standardized_attr_df, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = y_train.columns.size\n",
    "KXX = np.dot(X_train, X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = {}\n",
    "for annotator in y_train.columns:\n",
    "    Y[annotator] = pd.get_dummies(y_train[annotator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "MOnes = np.ones((m,1))\n",
    "aux_1 = np.eye(m) - np.dot(MOnes,(MOnes.T)/m)\n",
    "KXXc = np.dot(aux_1, KXX, aux_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrobeniusProduct(A,B):\n",
    "    return np.dot(A.T,B).trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "KYYc = {}\n",
    "KYY = {}\n",
    "a = np.array([])\n",
    "for annotator in y_train.columns:\n",
    "    KYY[annotator] = np.dot(Y[annotator], Y[annotator].T)\n",
    "    KYYc[annotator] = aux_1 * KYY[annotator] * aux_1\n",
    "    a = np.append(a, FrobeniusProduct(KYYc[annotator], KXXc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((annotators, annotators))\n",
    "for idA, annotator_a in enumerate(y_train.columns):\n",
    "    for idB, annotator_b in enumerate(y_train.columns):\n",
    "        M[idB, idA] = FrobeniusProduct(KYYc[annotator_b], KYYc[annotator_a]); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2*M\n",
    "f = -2*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4449998  0.44700595 0.44693058 0.45030136 0.44681382]\n"
     ]
    }
   ],
   "source": [
    "import osqp\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# Define problem data\n",
    "P = sparse.csc_matrix(H)\n",
    "q = f\n",
    "A = sparse.csc_matrix(-np.eye(annotators))\n",
    "l = np.zeros((annotators,1))\n",
    "u = np.ones((annotators,1))\n",
    "\n",
    "# Create an OSQP object\n",
    "prob = osqp.OSQP()\n",
    "\n",
    "# Setup workspace and change alpha parameter\n",
    "prob.setup(P, q, A, l, u, alpha=1.0, verbose=False)\n",
    "\n",
    "# Solve problem\n",
    "results = prob.solve()\n",
    "\n",
    "v = results.x\n",
    "mu = v/LA.norm(v)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0487e+07 -1.0487e+07  5e+00  2e+00  6e-10\n",
      " 1: -1.0487e+07 -1.0487e+07  9e-02  4e-02  1e-11\n",
      " 2: -1.0487e+07 -1.0487e+07  3e-02  2e-02  4e-12\n",
      " 3: -1.0487e+07 -1.0487e+07  3e-02  2e-02  4e-12\n",
      " 4: -8.9825e+06 -9.5036e+06  3e+06  2e-02  4e-12\n",
      " 5:  3.0743e+07 -3.2956e+07  8e+07  1e-02  4e-12\n",
      " 6:  2.2297e+07 -3.3668e+07  9e+07  1e-02  4e-12\n",
      " 7:  2.4592e+07 -3.7573e+07  1e+08  1e-02  3e-12\n",
      " 8:  1.4858e+06 -1.5440e+07  2e+07  4e-18  3e-15\n",
      " 9: -2.3094e+06 -8.4176e+06  6e+06  1e-18  9e-16\n",
      "10: -5.1784e+06 -1.3643e+07  8e+06  4e-18  4e-16\n",
      "11: -6.7362e+06 -7.3102e+06  6e+05  9e-19  1e-16\n",
      "12: -6.9647e+06 -6.9829e+06  2e+04  9e-19  1e-16\n",
      "13: -6.9795e+06 -6.9797e+06  2e+02  9e-19  1e-16\n",
      "14: -6.9797e+06 -6.9797e+06  2e+00  6e-23  1e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "from numpy import linalg as LA\n",
    "\n",
    "P = matrix(H)\n",
    "q = matrix(f)\n",
    "G = -matrix(np.eye(annotators))\n",
    "h = matrix(np.zeros((annotators,1)))\n",
    "\n",
    "sol = solvers.qp(P,q,G,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sol['x']\n",
    "mu = np.array(v/LA.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for annotator in y_train.columns:\n",
    "    models[annotator] = train_model(X_train, y_train[annotator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weight = np.zeros((X_test.shape[0],1))\n",
    "for idx, annotator_a in enumerate(y_train.columns):\n",
    "    probs = models[annotator].predict_proba(X_test)\n",
    "    true_class_prob = probs[:,0]\n",
    "    annotator_weight = (true_class_prob*mu[idx]).reshape((X_test.shape[0],1))\n",
    "    final_weight += annotator_weight\n",
    "final_weight = final_weight/mu.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [True if item >= 0.5 else False for item in final_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Precision:\n",
      "0.9122807017543859\n",
      "\n",
      "\n",
      "General Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.93      0.94      0.93       108\n",
      "          no       0.89      0.87      0.88        63\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.91      0.90      0.91       171\n",
      "weighted avg       0.91      0.91      0.91       171\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      "     0   1\n",
      "0  101   7\n",
      "1    8  55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "print(\"Global Precision:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"General Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['yes','no']))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "table = pd.DataFrame(matriz_confusion)\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
